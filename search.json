[
  {
    "objectID": "posts/pytorch_gradients.html",
    "href": "posts/pytorch_gradients.html",
    "title": "Pytorch Grad!",
    "section": "",
    "text": "import torch\n\n\n# We want to find a 'w' such that y is close to w * x.\n# The true 'w' is clearly 2.\nX = torch.tensor([1.0, 2.0, 3.0, 4.0], dtype=torch.float32)\nY = torch.tensor([2.0, 4.0, 6.0, 8.0], dtype=torch.float32)\n\n# Initialize our weight 'w' with a random guess.\n# requires_grad=True tells PyTorch to track this tensor for gradient calculations.\nw = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n\n\ndef forward(x):\n    return w * x\n\ndef loss(y, y_predicted):\n    return ((y_predicted - y)**2).mean()\n\nlearning_rate = 0.01\nn_iters = 20\n\n\n\nprint(f\"Starting training... Initial weight w = {w.item():.3f}\")\n\nfor epoch in range(n_iters):\n    # Forward pass\n    y_pred = forward(X)\n\n    # loss\n    l = loss(Y, y_pred)\n\n    # Calculate gradients = backward pass\n    # core of autograd. It calculates the derivative of 'l'\n    # with respect to every tensor that has requires_grad=True (i.e., 'w').\n    l.backward() # dl/dw\n\n    # Manually update weights\n    # torch.no_grad() because this is not part of the computation graph\n    with torch.no_grad():\n        # The calculated gradient is now in w.grad\n        # Update rule: w = w - learning_rate * gradient\n        w.data -= learning_rate * w.grad\n\n    # Zero out the gradients for the next iteration\n    if (epoch + 1) % 2 == 0:\n        print(f'Epoch {epoch+1}: w = {w.item():.3f}, w gradient: {w.grad}, loss = {l.item():.8f}')\n\n    w.grad.zero_()\n\n    # if (epoch + 1) % 2 == 0:\n    #     print(f'Epoch {epoch+1}: w = {w.item():.3f}, w gradient: {w.grad}, loss = {l.item():.8f}')\n\nprint(f\"\\nTraining finished. The learned weight is: {w.item():.3f}\")\nprint(\"The true weight was 2.0\")\n\nStarting training... Initial weight w = 0.000\nEpoch 2: w = 0.555, w gradient: -25.5, loss = 21.67499924\nEpoch 4: w = 0.956, w gradient: -18.423751831054688, loss = 11.31448650\nEpoch 6: w = 1.246, w gradient: -13.311159133911133, loss = 5.90623236\nEpoch 8: w = 1.455, w gradient: -9.61731243133545, loss = 3.08308983\nEpoch 10: w = 1.606, w gradient: -6.948507308959961, loss = 1.60939169\nEpoch 12: w = 1.716, w gradient: -5.020296096801758, loss = 0.84011245\nEpoch 14: w = 1.794, w gradient: -3.627163887023926, loss = 0.43854395\nEpoch 16: w = 1.851, w gradient: -2.6206254959106445, loss = 0.22892261\nEpoch 18: w = 1.893, w gradient: -1.8934016227722168, loss = 0.11949898\nEpoch 20: w = 1.922, w gradient: -1.3679819107055664, loss = 0.06237914\n\nTraining finished. The learned weight is: 1.922\nThe true weight was 2.0"
  },
  {
    "objectID": "posts/basic_deep_learning.html",
    "href": "posts/basic_deep_learning.html",
    "title": "Deep Learning Concepts",
    "section": "",
    "text": "The Perceptron is the simplest form of a neural network. It’s a single neuron that takes binary inputs, applies weights and a bias, and uses a step function to produce a binary output. It can only solve linearly separable problems.\nThe formula is: \\(y = f(\\sum_{i} w_i x_i + b)\\), where \\(f\\) is a step function.\n\\(f(x) = \\begin{cases} 1 & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x &lt; 0 \\end{cases}\\)\n\nimport matplotlib.pyplot as plt \n\n\nimport numpy as np\n\nclass Perceptron:\n    \"\"\"A simple Perceptron classifier.\"\"\"\n    def __init__(self, learning_rate=0.1, n_iters=100):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.activation_func = self._step_function\n        self.weights = None\n        self.bias = None\n\n    def _step_function(self, x):\n        return np.where(x &gt;= 0, 1, 0)\n\n    def fit(self, X, y):\n        print('Beginning to fit')\n        n_samples, n_features = X.shape\n        # Initialize weights and bias\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        for i in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                linear_output = np.dot(x_i, self.weights) + self.bias\n                y_predicted = self.activation_func(linear_output)\n\n                # Perceptron update rule\n                update = self.lr * (y[idx] - y_predicted)\n                self.weights += update * x_i\n                self.bias += update\n\n            if i%10==0:\n                print(i)\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.weights) + self.bias\n        return self.activation_func(linear_output)\n\n    def show(self):\n        fig, ax = plt.subplots(figsize=(4, 2))\n        ax.axis('off')\n\n        # Input layer (2 inputs)\n        ax.add_patch(plt.Circle((0.5, 1), 0.1, color='skyblue', ec='black'))\n        ax.text(0.3, 1, \"$x_1$\", fontsize=12)\n        ax.add_patch(plt.Circle((0.5, 0.5), 0.1, color='skyblue', ec='black'))\n        ax.text(0.3, 0.5, \"$x_2$\", fontsize=12)\n\n        # Output neuron\n        ax.add_patch(plt.Circle((2, 0.75), 0.12, color='salmon', ec='black'))\n        ax.text(2.2, 0.75, \"$\\hat{y}$\", fontsize=12)\n\n        # Arrows\n        ax.annotate(\"\", xy=(1.88, 0.75), xytext=(0.6, 1), arrowprops=dict(arrowstyle='-&gt;'))\n        ax.annotate(\"\", xy=(1.88, 0.75), xytext=(0.6, 0.5), arrowprops=dict(arrowstyle='-&gt;'))\n\n        ax.set_title(\"Perceptron Architecture\", fontsize=14)\n        plt.xlim(0, 2.5)\n        plt.ylim(0.2, 1.3)\n        plt.tight_layout()\n        plt.show()\n\n&lt;&gt;:51: SyntaxWarning: invalid escape sequence '\\h'\n&lt;&gt;:51: SyntaxWarning: invalid escape sequence '\\h'\n/tmp/ipykernel_65160/2482501332.py:51: SyntaxWarning: invalid escape sequence '\\h'\n  ax.text(2.2, 0.75, \"$\\hat{y}$\", fontsize=12)\n\n\n\nmodel = Perceptron()\n\n\nmodel.show()\n\n\n\n\n\n\n\n\n\n# Generate data \nx_vals = np.linspace(-5, 5, 500)\ny_step = model._step_function(x_vals)\n\n# Create the plot\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(x_vals, y_step, label='Step Function')\nplt.title('Step Activation Function')\nplt.xlabel('Input value (x)')\nplt.ylabel('Output value')\nplt.ylim(-0.1, 1.1)\nplt.legend()\nprint(\"Step Function: Outputs 0 for negative input, 1 for positive. Used in the original Perceptron.\")\n\nStep Function: Outputs 0 for negative input, 1 for positive. Used in the original Perceptron.\n\n\n\n\n\n\n\n\n\n\n\nThe XOR (exclusive OR) gate is a classic example of a non-linearly separable problem. A single straight line cannot separate the (0,1) and (1,0) points from (0,0) and (1,1).\n\n# XOR problem data\nX_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_xor = np.array([0, 1, 1, 0])\n\nplt.figure(figsize=(5, 5))\nfor label in np.unique(y_xor):\n    plt.scatter(\n        X_xor[y_xor == label, 0], \n        X_xor[y_xor == label, 1], \n        label=f\"Class {label}\", \n        edgecolor='k', \n        s=100\n    )\n\nplt.title(\"XOR Dataset\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.xticks([0, 1])\nplt.yticks([0, 1])\nplt.grid(True)\nplt.legend()\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Train the perceptron\nperceptron = Perceptron(learning_rate=0.1, n_iters=100)\nperceptron.fit(X_xor, y_xor)\n\nBeginning to fit\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n\n\n\nprint(\"\\n=== Perceptron Model Structure ===\")\nprint(f\"Number of layers: 1 (no hidden layer)\")\nprint(f\"Weights shape: {perceptron.weights.shape}\")\nprint(f\"Bias: {perceptron.bias}\")\n\n\n=== Perceptron Model Structure ===\nNumber of layers: 1 (no hidden layer)\nWeights shape: (2,)\nBias: 0.0\n\n\n\n# Get predictions\npredictions = perceptron.predict(X_xor)\n\nprint(f\"XOR Input:\\n{X_xor}\")\nprint(f\"Expected Output: {y_xor}\")\nprint(f\"Perceptron Output: {predictions}\")\naccuracy = np.sum(y_xor == predictions) / len(y_xor)\nprint(f\"Accuracy: {accuracy * 100}%\")\nprint(\"\\nAs you can see, the single-layer Perceptron cannot learn the XOR function.\")\n\nXOR Input:\n[[0 0]\n [0 1]\n [1 0]\n [1 1]]\nExpected Output: [0 1 1 0]\nPerceptron Output: [1 1 0 0]\nAccuracy: 50.0%\n\nAs you can see, the single-layer Perceptron cannot learn the XOR function."
  },
  {
    "objectID": "posts/basic_deep_learning.html#the-perceptron-a-single-linear-neuron",
    "href": "posts/basic_deep_learning.html#the-perceptron-a-single-linear-neuron",
    "title": "Deep Learning Concepts",
    "section": "",
    "text": "The Perceptron is the simplest form of a neural network. It’s a single neuron that takes binary inputs, applies weights and a bias, and uses a step function to produce a binary output. It can only solve linearly separable problems.\nThe formula is: \\(y = f(\\sum_{i} w_i x_i + b)\\), where \\(f\\) is a step function.\n\\(f(x) = \\begin{cases} 1 & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x &lt; 0 \\end{cases}\\)\n\nimport matplotlib.pyplot as plt \n\n\nimport numpy as np\n\nclass Perceptron:\n    \"\"\"A simple Perceptron classifier.\"\"\"\n    def __init__(self, learning_rate=0.1, n_iters=100):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.activation_func = self._step_function\n        self.weights = None\n        self.bias = None\n\n    def _step_function(self, x):\n        return np.where(x &gt;= 0, 1, 0)\n\n    def fit(self, X, y):\n        print('Beginning to fit')\n        n_samples, n_features = X.shape\n        # Initialize weights and bias\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        for i in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                linear_output = np.dot(x_i, self.weights) + self.bias\n                y_predicted = self.activation_func(linear_output)\n\n                # Perceptron update rule\n                update = self.lr * (y[idx] - y_predicted)\n                self.weights += update * x_i\n                self.bias += update\n\n            if i%10==0:\n                print(i)\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.weights) + self.bias\n        return self.activation_func(linear_output)\n\n    def show(self):\n        fig, ax = plt.subplots(figsize=(4, 2))\n        ax.axis('off')\n\n        # Input layer (2 inputs)\n        ax.add_patch(plt.Circle((0.5, 1), 0.1, color='skyblue', ec='black'))\n        ax.text(0.3, 1, \"$x_1$\", fontsize=12)\n        ax.add_patch(plt.Circle((0.5, 0.5), 0.1, color='skyblue', ec='black'))\n        ax.text(0.3, 0.5, \"$x_2$\", fontsize=12)\n\n        # Output neuron\n        ax.add_patch(plt.Circle((2, 0.75), 0.12, color='salmon', ec='black'))\n        ax.text(2.2, 0.75, \"$\\hat{y}$\", fontsize=12)\n\n        # Arrows\n        ax.annotate(\"\", xy=(1.88, 0.75), xytext=(0.6, 1), arrowprops=dict(arrowstyle='-&gt;'))\n        ax.annotate(\"\", xy=(1.88, 0.75), xytext=(0.6, 0.5), arrowprops=dict(arrowstyle='-&gt;'))\n\n        ax.set_title(\"Perceptron Architecture\", fontsize=14)\n        plt.xlim(0, 2.5)\n        plt.ylim(0.2, 1.3)\n        plt.tight_layout()\n        plt.show()\n\n&lt;&gt;:51: SyntaxWarning: invalid escape sequence '\\h'\n&lt;&gt;:51: SyntaxWarning: invalid escape sequence '\\h'\n/tmp/ipykernel_65160/2482501332.py:51: SyntaxWarning: invalid escape sequence '\\h'\n  ax.text(2.2, 0.75, \"$\\hat{y}$\", fontsize=12)\n\n\n\nmodel = Perceptron()\n\n\nmodel.show()\n\n\n\n\n\n\n\n\n\n# Generate data \nx_vals = np.linspace(-5, 5, 500)\ny_step = model._step_function(x_vals)\n\n# Create the plot\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(x_vals, y_step, label='Step Function')\nplt.title('Step Activation Function')\nplt.xlabel('Input value (x)')\nplt.ylabel('Output value')\nplt.ylim(-0.1, 1.1)\nplt.legend()\nprint(\"Step Function: Outputs 0 for negative input, 1 for positive. Used in the original Perceptron.\")\n\nStep Function: Outputs 0 for negative input, 1 for positive. Used in the original Perceptron.\n\n\n\n\n\n\n\n\n\n\n\nThe XOR (exclusive OR) gate is a classic example of a non-linearly separable problem. A single straight line cannot separate the (0,1) and (1,0) points from (0,0) and (1,1).\n\n# XOR problem data\nX_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_xor = np.array([0, 1, 1, 0])\n\nplt.figure(figsize=(5, 5))\nfor label in np.unique(y_xor):\n    plt.scatter(\n        X_xor[y_xor == label, 0], \n        X_xor[y_xor == label, 1], \n        label=f\"Class {label}\", \n        edgecolor='k', \n        s=100\n    )\n\nplt.title(\"XOR Dataset\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.xticks([0, 1])\nplt.yticks([0, 1])\nplt.grid(True)\nplt.legend()\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Train the perceptron\nperceptron = Perceptron(learning_rate=0.1, n_iters=100)\nperceptron.fit(X_xor, y_xor)\n\nBeginning to fit\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n\n\n\nprint(\"\\n=== Perceptron Model Structure ===\")\nprint(f\"Number of layers: 1 (no hidden layer)\")\nprint(f\"Weights shape: {perceptron.weights.shape}\")\nprint(f\"Bias: {perceptron.bias}\")\n\n\n=== Perceptron Model Structure ===\nNumber of layers: 1 (no hidden layer)\nWeights shape: (2,)\nBias: 0.0\n\n\n\n# Get predictions\npredictions = perceptron.predict(X_xor)\n\nprint(f\"XOR Input:\\n{X_xor}\")\nprint(f\"Expected Output: {y_xor}\")\nprint(f\"Perceptron Output: {predictions}\")\naccuracy = np.sum(y_xor == predictions) / len(y_xor)\nprint(f\"Accuracy: {accuracy * 100}%\")\nprint(\"\\nAs you can see, the single-layer Perceptron cannot learn the XOR function.\")\n\nXOR Input:\n[[0 0]\n [0 1]\n [1 0]\n [1 1]]\nExpected Output: [0 1 1 0]\nPerceptron Output: [1 1 0 0]\nAccuracy: 50.0%\n\nAs you can see, the single-layer Perceptron cannot learn the XOR function."
  },
  {
    "objectID": "posts/basic_deep_learning.html#multilayer-perceptron-mlp-for-xor",
    "href": "posts/basic_deep_learning.html#multilayer-perceptron-mlp-for-xor",
    "title": "Deep Learning Concepts",
    "section": "2. Multilayer Perceptron (MLP) for XOR",
    "text": "2. Multilayer Perceptron (MLP) for XOR\nTo solve non-linear problems like XOR, we need to add a hidden layer. This is a Multilayer Perceptron (MLP). The hidden layer allows the network to learn non-linear combinations of the inputs. We also switch to a smooth activation function like the Sigmoid function to enable gradient-based learning via backpropagation.\nMathematically,\nSigmoid function: \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\n\n# Activation function and its derivative\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\ny_sigmoid = sigmoid(x_vals)\ny_sigmoid_deriv = sigmoid_derivative(y_sigmoid)\n\nplt.figure(figsize=(6, 6))\nplt.plot(x_vals, y_sigmoid, label='Sigmoid')\nplt.plot(x_vals, y_sigmoid_deriv, label='Sigmoid Derivative', linestyle='--')\nplt.title('Sigmoid Function and its Derivative')\nplt.xlabel('Input value (x)')\nplt.ylabel('Output value')\nplt.legend()\n\n\n\n\n\n\n\n\n\ndef draw_mlp_architecture(input_size, hidden_size, output_size):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.axis('off')\n\n    # Circle radius\n    r = 0.1\n\n    # Layer x-positions\n    x_input = 0.5\n    x_hidden = 2\n    x_output = 3.5\n\n    # Draw input layer\n    for i in range(input_size):\n        y = 1.5 - i * 0.75\n        ax.add_patch(plt.Circle((x_input, y), r, color='skyblue', ec='black'))\n        ax.text(x_input - 0.3, y, f\"$x_{i+1}$\", fontsize=12)\n\n    # Draw hidden layer\n    for j in range(hidden_size):\n        y = 1.5 - j * 0.75\n        ax.add_patch(plt.Circle((x_hidden, y), r, color='lightgreen', ec='black'))\n        ax.text(x_hidden, y, f\"$h_{j+1}$\", fontsize=12, ha='center', va='center')\n\n    # Draw output layer\n    for k in range(output_size):\n        y = 0.75  # Always one output neuron here\n        ax.add_patch(plt.Circle((x_output, y), r, color='salmon', ec='black'))\n        ax.text(x_output + 0.2, y, \"$\\\\hat{y}$\", fontsize=12)\n\n    # Arrows from input to hidden\n    for i in range(input_size):\n        y1 = 1.5 - i * 0.75\n        for j in range(hidden_size):\n            y2 = 1.5 - j * 0.75\n            ax.annotate(\"\", xy=(x_hidden - r, y2), xytext=(x_input + r, y1),\n                        arrowprops=dict(arrowstyle='-&gt;', lw=1))\n\n    # Arrows from hidden to output\n    for j in range(hidden_size):\n        y2 = 1.5 - j * 0.75\n        y_out = 0.75\n        ax.annotate(\"\", xy=(x_output - r, y_out), xytext=(x_hidden + r, y2),\n                    arrowprops=dict(arrowstyle='-&gt;', lw=1))\n\n    ax.set_title(\"MLP Architecture for XOR\", fontsize=14)\n    plt.xlim(0, 4)\n    plt.ylim(-0.5, 2.0)\n    plt.tight_layout()\n    plt.show()\n\n\nimport numpy as np\n\nclass MLP_XOR:\n    def __init__(self, input_size=2, hidden_size=2, output_size=1):\n        # Initialize weights randomly to break symmetry\n        self.weights_hidden = np.random.uniform(size=(input_size, hidden_size))\n        self.weights_output = np.random.uniform(size=(hidden_size, output_size))\n        \n        # Biases can be initialized to zero or randomly\n        self.bias_hidden = np.random.uniform(size=(1, hidden_size))\n        self.bias_output = np.random.uniform(size=(1, output_size))\n\n    def forward(self, X):\n        # Forward propagation\n        self.hidden_activation = sigmoid(np.dot(X, self.weights_hidden) + self.bias_hidden)\n        self.output = sigmoid(np.dot(self.hidden_activation, self.weights_output) + self.bias_output)\n        return self.output\n\n    def backward(self, X, y, output, lr):\n        # error\n        output_error = y - output\n        output_delta = output_error * sigmoid_derivative(output)\n\n        hidden_error = output_delta.dot(self.weights_output.T)\n        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_activation)\n\n        # Update weights and biases\n        self.weights_output += self.hidden_activation.T.dot(output_delta) * lr\n        self.weights_hidden += X.T.dot(hidden_delta) * lr\n        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * lr\n        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * lr\n\n    def train(self, X, y, epochs=10000, lr=0.1):\n        y = y.reshape(-1, 1) # Ensure y is a column vector\n        for i in range(epochs):\n            output = self.forward(X)\n            self.backward(X, y, output, lr)\n            if (i % 1000) == 0:\n                loss = np.mean(np.square(y - output))\n                print(f\"Epoch {i} Loss: {loss:.4f}\")\n\n    def predict(self, X):\n        return (self.forward(X) &gt; 0.5).astype(int)\n\n    def show(self):\n        draw_mlp_architecture(\n            input_size=self.weights_hidden.shape[0],\n            hidden_size=self.weights_hidden.shape[1],\n            output_size=self.weights_output.shape[1]\n        )\n\n\nmlp = MLP_XOR()\nmlp.show()\n\n\n\n\n\n\n\n\n\n\nX_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_xor = np.array([0, 1, 1, 0])\n\nmlp_xor = MLP_XOR()\nmlp_xor.train(X_xor, y_xor)\n\nEpoch 0 Loss: 0.3274\nEpoch 1000 Loss: 0.2498\nEpoch 2000 Loss: 0.2479\nEpoch 3000 Loss: 0.2306\nEpoch 4000 Loss: 0.1784\nEpoch 5000 Loss: 0.0817\nEpoch 6000 Loss: 0.0219\nEpoch 7000 Loss: 0.0104\nEpoch 8000 Loss: 0.0065\nEpoch 9000 Loss: 0.0046\n\n\n\npredictions = mlp_xor.predict(X_xor)\n\nprint(\"\\n--- MLP for XOR Results ---\")\nprint(f\"Expected Output: {y_xor}\")\nprint(f\"MLP Final Output: {predictions.flatten()}\")\naccuracy = np.sum(y_xor == predictions.flatten()) / len(y_xor)\nprint(f\"Accuracy: {accuracy * 100}%\")\nprint(\"\\nSuccess! The MLP with a hidden layer correctly learns the XOR function.\")\n\n\n--- MLP for XOR Results ---\nExpected Output: [0 1 1 0]\nMLP Final Output: [0 1 1 0]\nAccuracy: 100.0%\n\nSuccess! The MLP with a hidden layer correctly learns the XOR function."
  },
  {
    "objectID": "posts/basic_deep_learning.html#simple-neural-network-for-mnist-from-scratch",
    "href": "posts/basic_deep_learning.html#simple-neural-network-for-mnist-from-scratch",
    "title": "Deep Learning Concepts",
    "section": "3. Simple Neural Network for MNIST from Scratch",
    "text": "3. Simple Neural Network for MNIST from Scratch\nNow, we’ll scale up to a more complex problem: classifying handwritten digits from the MNIST dataset. We will build everything from scratch.\n\nArchitecture: Input Layer (784 neurons) -&gt; Hidden Layer (128 neurons, ReLU activation) -&gt; Output Layer (10 neurons, Softmax activation)\nLoss Function: Categorical Cross-Entropy\nOptimizer: Stochastic Gradient Descent (SGD)\n\nNote: We use torchvision for convenience to download and load the dataset, but all network logic is pure NumPy.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\n\n\ntransform = transforms.ToTensor()\ntrain_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\ntest_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\nlen(train_data), len(test_data)\n\n(60000, 10000)\n\n\n\n# convert to numpy\n# flatten the images\n# normalize the data\nprint(train_data.data.numpy().shape)\n\nX_train = train_data.data.numpy().reshape(len(train_data), -1) / 255.0\ny_train_raw = train_data.targets.numpy()\n\nX_test = test_data.data.numpy().reshape(len(test_data), -1) / 255.0\ny_test_raw = test_data.targets.numpy()\n\nX_train.shape\n\n(60000, 28, 28)\n\n\n(60000, 784)\n\n\n\n# One-hot encode labels\ndef one_hot(y, num_classes):\n    return np.eye(num_classes)[y]\n\n\n# demonstrating one-hot\nlabel = 7\nbatch_of_labels = np.array([3, 0, 9, 1])\nnum_classes = 10\n\none_hot_label = one_hot(label, num_classes)\none_hot_batch = one_hot(batch_of_labels, num_classes)\n\nprint(f\"Original label: {label}\")\nprint(f\"One-hot vector: {one_hot_label}\\n\")\n\nprint(f\"Original batch: {batch_of_labels}\")\nprint(f\"One-hot batch:\\n{one_hot_batch}\")\n\nOriginal label: 7\nOne-hot vector: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n\nOriginal batch: [3 0 9 1]\nOne-hot batch:\n[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n\n\ny_train = one_hot(y_train_raw, 10)\ny_test = one_hot(y_test_raw, 10)\n\ny_train[:2, :], y_test[:2, :]\n\n(array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]))\n\n\n\n\nprint(f\"Training data shape: {X_train.shape}\")\nprint(f\"Training labels shape: {y_train.shape}\")\n\nTraining data shape: (60000, 784)\nTraining labels shape: (60000, 10)\n\n\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    return np.where(x &gt; 0, 1, 0)\n\ny_relu = relu(x_vals)\ny_relu_deriv = relu_derivative(x_vals)\n\nplt.plot(x_vals, y_relu, label='ReLU')\nplt.plot(x_vals, y_relu_deriv, label='ReLU Derivative', linestyle='--')\nplt.title('ReLU Function and its Derivative')\nplt.xlabel('Input value (x)')\nplt.ylabel('Output value')\nplt.legend()\n\n\n\n\n\n\n\n\n\ndef softmax(x):\n    exps = np.exp(x - np.max(x, axis=1, keepdims=True)) # difference for stability\n    return exps / np.sum(exps, axis=1, keepdims=True)\n\n# extra bracket for batch dimension\nlogits = np.array([[2.0, 1.0, 0.1, 3.0, -1.0]]) \n\nprobabilities = softmax(logits)\n\n# flatten to plot\nprobabilities_flatten = probabilities.flatten()\n\nprint(f\"Original Logits: {logits.flatten()}\")\nprint(f\"Probabilities after Softmax: {np.round(probabilities, 3)}\")\nprint(f\"Sum of probabilities: {np.sum(probabilities):.2f}\")\n\nclass_indices = [f'Class {i}' for i in range(len(probabilities_flatten))]\nplt.bar(class_indices, probabilities_flatten)\nplt.title('Softmax Function Output')\nplt.xlabel('Class')\nplt.ylabel('Probability')\nplt.ylim(0, 1)\nprint(\"Softmax Function: Converts raw scores (logits) into a probability distribution. The class with the highest logit gets the highest probability.\")\n\nOriginal Logits: [ 2.   1.   0.1  3.  -1. ]\nProbabilities after Softmax: [[0.233 0.086 0.035 0.634 0.012]]\nSum of probabilities: 1.00\nSoftmax Function: Converts raw scores (logits) into a probability distribution. The class with the highest logit gets the highest probability.\n\n\n\n\n\n\n\n\n\n\n# penalty should grow exponentially as the model gets more confident and wrong\ndef cross_entropy_loss(y_pred, y_true):\n    # y_true contains labels for entire batch \n    # Clip to avoid log(0)\n    y_pred_clipped = np.clip(y_pred, 1e-12, 1. - 1e-12)\n\n    # so divided by batch size for averaging loss per sample\n    # y_true is either 0 or 1, one_hot\n    return -np.sum(y_true * np.log(y_pred_clipped)) / y_true.shape[0]\n\n\ny_true = np.array([[0, 0, 1, 0]])\n\npredicted_probs_for_correct_class = np.linspace(0.01, 0.99, 200)\n\n# basic curve\nlosses_curve = [-np.log(p) for p in predicted_probs_for_correct_class]\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.figure(figsize=(10, 6))\nplt.plot(predicted_probs_for_correct_class, losses_curve, color='royalblue', label='Loss Curve')\n\n# 3 Key Cases \ncases = {\n    'A': 0.95, # High Confidence, Correct\n    'B': 0.50, # Medium Confidence\n    'C': 0.05  # Low Confidence, Wrong\n}\n\ncolors = {'A': 'green', 'B': 'orange', 'C': 'red'}\nprint(\"--- Predictions and Losses for 3 Cases ---\\n\")\n\nfor case, prob in cases.items():\n    remaining_prob = (1 - prob) / 3\n\n    # sharing same for the rest of the 3 classes\n    y_pred = np.array([remaining_prob, remaining_prob, prob, remaining_prob])\n    loss = cross_entropy_loss(y_pred.reshape(1, -1), y_true)\n\n    print(f\"--- Case {case} ---\")\n    print(f\"Prediction Vector (y_pred): {np.round(y_pred, 4)}\")\n    print(f\"Corresponding Loss: {loss:.4f}\\n\")\n\n    # Marking the points\n    plt.plot(prob, loss, 'o', color=colors[case], markersize=10, label=f'Case {case}')\n\nplt.title('Cross-Entropy Loss Curve', fontsize=16)\nplt.xlabel('Predicted Probability for the Correct Class', fontsize=12)\nplt.ylabel('Calculated Loss', fontsize=12)\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 5)\nplt.show()\n\n--- Predictions and Losses for 3 Cases ---\n\n--- Case A ---\nPrediction Vector (y_pred): [0.0167 0.0167 0.95   0.0167]\nCorresponding Loss: 0.0513\n\n--- Case B ---\nPrediction Vector (y_pred): [0.1667 0.1667 0.5    0.1667]\nCorresponding Loss: 0.6931\n\n--- Case C ---\nPrediction Vector (y_pred): [0.3167 0.3167 0.05   0.3167]\nCorresponding Loss: 2.9957\n\n\n\n\n\n\n\n\n\n\n\nSimple computation graph to understand gradients formula computation\n\n\n\nbackprop.png\n\n\nCross entropy backpropagation: https://medium.com/data-science/deriving-backpropagation-with-cross-entropy-loss-d24811edeaf9\n\nclass SimpleNN_MNIST:\n    def __init__(self, input_size, hidden_size, output_size):\n        # He initialization for weights\n        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n        self.b1 = np.zeros((1, hidden_size))\n        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n        self.b2 = np.zeros((1, output_size))\n        \n    def forward(self, X):\n        # Store intermediate values for backpropagation\n        self.Z1 = X @ self.W1 + self.b1\n        self.A1 = relu(self.Z1)\n        self.Z2 = self.A1 @ self.W2 + self.b2\n        self.A2 = softmax(self.Z2)\n        return self.A2\n    \n    def backward(self, X, y_true):\n        # Number of samples in the batch\n        m = y_true.shape[0] \n    \n        # -----------------------------------\n        # Output Layer Gradients\n        # -----------------------------------\n    \n        # Gradient of the loss with respect to Z2 (pre-activation of the output layer)\n        # Since we're using Softmax + Cross-Entropy Loss, the gradient simplifies to:\n        # dZ2 = A2 - y_true\n        # A2 is the output from softmax, y_true is one-hot encoded ground truth\n        dZ2 = self.A2 - y_true\n    \n        # Gradient of the loss with respect to W2 (weights between hidden and output layers)\n        # Using the chain rule: dW2 = (A1^T @ dZ2) / m\n        # A1: activations from hidden layer, shape (m, hidden_dim)\n        # dZ2: error term for output layer, shape (m, output_dim)\n        # A1.T @ dZ2 results in shape (hidden_dim, output_dim)\n        self.dW2 = (self.A1.T @ dZ2) / m\n    \n        # Gradient of the loss with respect to b2 (bias of the output layer)\n        # Sum over the batch dimension to get bias gradient: shape (1, output_dim)\n        self.db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n\n    \n        # Backpropagating the error to the hidden layer\n        # dA1 = dZ2 @ W2^T\n        # W2.T: shape (output_dim, hidden_dim)\n        # dZ2: shape (m, output_dim)\n        # dA1: shape (m, hidden_dim), error signal for hidden layer outputs (A1)\n        dA1 = dZ2 @ self.W2.T\n    \n        # Applying the derivative of the ReLU activation function\n        # ReLU'(Z1) is 1 where Z1 &gt; 0, else 0\n        # Element-wise multiply with dA1 to get dZ1 (gradient wrt pre-activation of hidden layer)\n        dZ1 = dA1 * relu_derivative(self.Z1)\n        self.dW1 = (X.T @ dZ1) / m\n        self.db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n\n    def update_params(self, lr):\n        # Basic SGD optimizer\n        self.W1 -= lr * self.dW1\n        self.b1 -= lr * self.db1\n        self.W2 -= lr * self.dW2\n        self.b2 -= lr * self.db2\n        \n    def train(self, X_train, y_train, X_test, y_test_raw, epochs, lr, batch_size):\n        history = {'loss': [], 'accuracy': []}\n        num_batches = len(X_train) // batch_size\n        \n        for epoch in range(epochs):\n            # Shuffle \n            permutation = np.random.permutation(len(X_train))\n            X_train_shuffled = X_train[permutation]\n            y_train_shuffled = y_train[permutation]\n            \n            epoch_loss = 0\n            for i in tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{epochs}\"):\n                # Create mini-batch\n                start = i * batch_size\n                end = start + batch_size\n                X_batch = X_train_shuffled[start:end]\n                y_batch = y_train_shuffled[start:end]\n                \n                y_pred = self.forward(X_batch)\n                epoch_loss += cross_entropy_loss(y_pred, y_batch)\n                self.backward(X_batch, y_batch)\n                self.update_params(lr)\n            \n            # Calculate loss and accuracy at the end of epoch\n            avg_loss = epoch_loss / num_batches\n            \n            # Evaluate on test set\n            y_pred_test = self.predict(X_test)\n            accuracy = np.sum(y_pred_test == y_test_raw) / len(y_test_raw)\n            \n            history['loss'].append(avg_loss)\n            history['accuracy'].append(accuracy)\n            print(f'Epoch {epoch+1} - Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n        return history\n\n    def predict(self, X):\n        y_pred_probs = self.forward(X)\n        return np.argmax(y_pred_probs, axis=1)\n\n\n# --- 3. Train the Network and Plot Results ---\n\n# Hyperparameters\nINPUT_SIZE = 784\nHIDDEN_SIZE = 128\nOUTPUT_SIZE = 10\nEPOCHS = 10\nLEARNING_RATE = 0.1\nBATCH_SIZE = 64\n\nscratch_nn = SimpleNN_MNIST(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\nhistory = scratch_nn.train(X_train, y_train, X_test, y_test_raw, EPOCHS, LEARNING_RATE, BATCH_SIZE)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\nfig.suptitle('From-Scratch Model Training', fontsize=16)\n\nax1.plot(history['loss'])\nax1.set_title('Training Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Cross-Entropy Loss')\n\nax2.plot(history['accuracy'])\nax2.set_title('Test Accuracy')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy')\n\nplt.show()\n\nEpoch 1/10: 100%|██████████████████████| 937/937 [00:02&lt;00:00, 315.13it/s]\n\n\nEpoch 1 - Loss: 0.3735, Test Accuracy: 0.9359\n\n\nEpoch 2/10: 100%|██████████████████████| 937/937 [00:01&lt;00:00, 536.68it/s]\n\n\nEpoch 2 - Loss: 0.2000, Test Accuracy: 0.9515\n\n\nEpoch 3/10: 100%|██████████████████████| 937/937 [00:02&lt;00:00, 428.29it/s]\n\n\nEpoch 3 - Loss: 0.1509, Test Accuracy: 0.9600\n\n\nEpoch 4/10: 100%|██████████████████████| 937/937 [00:01&lt;00:00, 569.65it/s]\n\n\nEpoch 4 - Loss: 0.1223, Test Accuracy: 0.9645\n\n\nEpoch 5/10: 100%|██████████████████████| 937/937 [00:01&lt;00:00, 546.63it/s]\n\n\nEpoch 5 - Loss: 0.1034, Test Accuracy: 0.9675\n\n\nEpoch 6/10: 100%|██████████████████████| 937/937 [00:03&lt;00:00, 308.86it/s]\n\n\nEpoch 6 - Loss: 0.0895, Test Accuracy: 0.9693\n\n\nEpoch 7/10: 100%|██████████████████████| 937/937 [00:01&lt;00:00, 546.72it/s]\n\n\nEpoch 7 - Loss: 0.0784, Test Accuracy: 0.9728\n\n\nEpoch 8/10: 100%|██████████████████████| 937/937 [00:01&lt;00:00, 528.95it/s]\n\n\nEpoch 8 - Loss: 0.0700, Test Accuracy: 0.9745\n\n\nEpoch 9/10: 100%|██████████████████████| 937/937 [00:02&lt;00:00, 467.59it/s]\n\n\nEpoch 9 - Loss: 0.0624, Test Accuracy: 0.9752\n\n\nEpoch 10/10: 100%|█████████████████████| 937/937 [00:01&lt;00:00, 492.77it/s]\n\n\nEpoch 10 - Loss: 0.0565, Test Accuracy: 0.9761"
  },
  {
    "objectID": "posts/basic_deep_learning.html#weight-initialization-techniques",
    "href": "posts/basic_deep_learning.html#weight-initialization-techniques",
    "title": "Deep Learning Concepts",
    "section": "4. Weight Initialization Techniques",
    "text": "4. Weight Initialization Techniques\nProper weight initialization is crucial for preventing gradients from vanishing (becoming too small) or exploding (becoming too large) during training. Here are a few common techniques implemented from scratch.\n\nZeros Initialization: A bad practice that causes all neurons in a layer to learn the same thing.\nRandom Normal: Breaks symmetry, but can lead to vanishing/exploding gradients if not scaled correctly.\nXavier/Glorot Initialization: Scales weights based on the number of input neurons (n_in). Good for Tanh/Sigmoid activations. Formula: \\(W \\sim N(0, \\sqrt{1/n_{in}})\\).\nHe Initialization: Scales weights based on n_in. Designed for ReLU-based activations. Formula: \\(W \\sim N(0, \\sqrt{2/n_{in}})\\).\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\n\n# Initialization \ndef zeros_init(n_in, n_out):\n    return np.zeros((n_out, n_in))\n\ndef random_normal_init(n_in, n_out):\n    return np.random.randn(n_out, n_in) * 0.01\n\ndef xavier_init(n_in, n_out):\n    return np.random.randn(n_out, n_in) * np.sqrt(1.0 / n_in)\n\ndef he_init(n_in, n_out):\n    return np.random.randn(n_out, n_in) * np.sqrt(2.0 / n_in)\n\n# plot density curves \ndef plot_density(weights, label, color):\n    flat_weights = weights.flatten()\n    density = gaussian_kde(flat_weights)\n    x_vals = np.linspace(flat_weights.min(), flat_weights.max(), 200)\n    plt.plot(x_vals, density(x_vals), label=label, color=color)\n\n# Layer dimensions\nn_in, n_out = 784, 128\n\ninitializations = {\n    \"Random Normal\": (random_normal_init(n_in, n_out), 'blue'),\n    \"Xavier\": (xavier_init(n_in, n_out), 'red'),\n    \"He\": (he_init(n_in, n_out), 'green'),\n    \"Zeros\": (zeros_init(n_in, n_out), 'black')  \n}\n\n# Print stats and plot densities (excluding Zeros)\nplt.figure(figsize=(10, 5))\nfor name, (weights, color) in initializations.items():\n    mean, std = weights.mean(), weights.std()\n    print(f\"{name:&lt;15} | Mean: {mean:&gt;7.4f}, Std: {std:&gt;7.4f}\")\n    if name != \"Zeros\":\n        plot_density(weights, name, color)\n\nplt.title(\"Weight Initialization Density (Excl. Zeros)\")\nplt.xlabel(\"Weight Value\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Plot Zeros separately\nplt.figure(figsize=(5, 4))\nplt.hist(initializations[\"Zeros\"][0].flatten(), bins=10, color='gray')\nplt.title(\"Zeros Initialization (Separate View)\")\nplt.xlabel(\"Weight Value\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nRandom Normal   | Mean:  0.0000, Std:  0.0100\nXavier          | Mean:  0.0000, Std:  0.0358\nHe              | Mean:  0.0000, Std:  0.0505\nZeros           | Mean:  0.0000, Std:  0.0000"
  },
  {
    "objectID": "posts/basic_deep_learning.html#pytorch-verification",
    "href": "posts/basic_deep_learning.html#pytorch-verification",
    "title": "Deep Learning Concepts",
    "section": "5. PyTorch Verification",
    "text": "5. PyTorch Verification\nLet’s build the exact same network in PyTorch. This helps verify that our from-scratch implementation is correct. We will use the same architecture, hyperparameters, and optimizer.\nThe final accuracy should be very close to our NumPy model.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train_raw, dtype=torch.long) \nX_test_t = torch.tensor(X_test, dtype=torch.float32)\ny_test_t = torch.tensor(y_test_raw, dtype=torch.long)\n\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\ntest_dataset = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nclass PyTorchNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(PyTorchNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        \n        # Apply He initialization\n        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out \n\n\npytorch_nn = PyTorchNN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\ncriterion = nn.CrossEntropyLoss() \noptimizer = optim.SGD(pytorch_nn.parameters(), lr=LEARNING_RATE)\n\npytorch_history = {'loss': [], 'accuracy': []}\n\nfor epoch in range(EPOCHS):\n    epoch_loss = 0\n    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")):\n        outputs = pytorch_nn(inputs)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    \n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            outputs = pytorch_nn(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    avg_loss = epoch_loss / len(train_loader)\n    accuracy = correct / total\n    \n    pytorch_history['loss'].append(avg_loss)\n    pytorch_history['accuracy'].append(accuracy)\n    print(f'Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n\nEpoch 1/10: 100%|█████████████████████| 938/938 [00:00&lt;00:00, 1080.91it/s]\n\n\nEpoch 1 - Loss: 0.3656, Accuracy: 0.9382\n\n\nEpoch 2/10: 100%|█████████████████████| 938/938 [00:00&lt;00:00, 1130.23it/s]\n\n\nEpoch 2 - Loss: 0.1950, Accuracy: 0.9547\n\n\nEpoch 3/10: 100%|█████████████████████| 938/938 [00:00&lt;00:00, 1095.94it/s]\n\n\nEpoch 3 - Loss: 0.1472, Accuracy: 0.9615\n\n\nEpoch 4/10: 100%|██████████████████████| 938/938 [00:00&lt;00:00, 996.78it/s]\n\n\nEpoch 4 - Loss: 0.1205, Accuracy: 0.9628\n\n\nEpoch 5/10: 100%|██████████████████████| 938/938 [00:01&lt;00:00, 853.66it/s]\n\n\nEpoch 5 - Loss: 0.1021, Accuracy: 0.9704\n\n\nEpoch 6/10: 100%|██████████████████████| 938/938 [00:01&lt;00:00, 857.51it/s]\n\n\nEpoch 6 - Loss: 0.0882, Accuracy: 0.9725\n\n\nEpoch 7/10: 100%|██████████████████████| 938/938 [00:00&lt;00:00, 979.19it/s]\n\n\nEpoch 7 - Loss: 0.0776, Accuracy: 0.9717\n\n\nEpoch 8/10: 100%|██████████████████████| 938/938 [00:01&lt;00:00, 842.42it/s]\n\n\nEpoch 8 - Loss: 0.0690, Accuracy: 0.9751\n\n\nEpoch 9/10: 100%|█████████████████████| 938/938 [00:00&lt;00:00, 1029.96it/s]\n\n\nEpoch 9 - Loss: 0.0624, Accuracy: 0.9755\n\n\nEpoch 10/10: 100%|█████████████████████| 938/938 [00:00&lt;00:00, 951.11it/s]\n\n\nEpoch 10 - Loss: 0.0562, Accuracy: 0.9775\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\nfig.suptitle('From-Scratch vs PyTorch Model Comparison', fontsize=16)\n\nax1.plot(history['loss'], label='From Scratch')\nax1.plot(pytorch_history['loss'], label='PyTorch', linestyle='--')\nax1.set_title('Training Loss Comparison')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Cross-Entropy Loss')\nax1.legend()\n\nax2.plot(history['accuracy'], label='From Scratch')\nax2.plot(pytorch_history['accuracy'], label='PyTorch', linestyle='--')\nax2.set_title('Test Accuracy Comparison')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy')\nax2.legend()\n\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A learning journal — sharing what I discover, build, and experiment with."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs",
    "section": "",
    "text": "Deep Learning Concepts\n\n\n\nmachine-learning\n\ntutorials\n\ndeep-learning\n\n\n\nA simple intro to deep learning from scratch, and using Pytorch.\n\n\n\n\n\nAug 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning in Pytorch\n\n\n\nmachine-learning\n\ntutorials\n\ndeep-learning\n\npytorch\n\nsvhn\n\n\n\nFull training script of a deep learning model using pytorch\n\n\n\n\n\nAug 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Grad!\n\n\n\nmachine-learning\n\ntutorials\n\ndeep-learning\n\npytorch\n\n\n\nHow Gradients Work in Pytorch; A simple short implementation\n\n\n\n\n\nAug 19, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/full_workflow_dl.html",
    "href": "posts/full_workflow_dl.html",
    "title": "Deep Learning in Pytorch",
    "section": "",
    "text": "Basic Intro\nThis blog depicts a basic CNN model used for image classification using SVHN dataset. The objective is to give an overview of how datasets are required to be loaded, how model architectures are to be made using pytorch library, and how overall training works.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport numpy as np\nimport matplotlib.pyplot as plt \n\n\n# Define transformations for the SVHN dataset\n# Convert images to PyTorch Tensors\n# Normalize the tensors. The values (0.5, 0.5, 0.5) are standard for normalizing to [-1, 1]\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrain_dataset = torchvision.datasets.SVHN(root='./data', split='train',\n                                        download=True, transform=transform)\n\nval_dataset = torchvision.datasets.SVHN(root='./data', split='test',\n                                       download=True, transform=transform)\n\nlen(train_dataset), len(val_dataset)\n\n(73257, 26032)\n\n\n\ndef imshow(img, title=None):\n    img = img / 2 + 0.5  # unnormalize [-1, 1] -&gt; [0, 1]\n    np_img = img.numpy()\n    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n    if title:\n        plt.title(title)\n    plt.axis(\"off\")\n\nprint(\"Random samples from SVHN training set:\")\nfig, axes = plt.subplots(1, 5, figsize=(12, 3))\nfor i in range(5):\n    idx = torch.randint(0, len(train_dataset), (1,)).item()\n    img, label = train_dataset[idx]\n    plt.subplot(1, 5, i+1)\n    imshow(img)\n    plt.title(f\"Label: {label}\")\n    plt.axis(\"off\")\nplt.show()\n\nRandom samples from SVHN training set:\n\n\n\n\n\n\n\n\n\n\nBATCH_SIZE = 64\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\nprint(f\"Data loaded. Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\\n\")\n\nData loaded. Training samples: 73257, Validation samples: 26032\n\n\n\n\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\n# Show the batch\nprint(\"A batch from the DataLoader:\")\nimshow(torchvision.utils.make_grid(images, nrow=8),\n       title=\" \".join(str(label.item()) for label in labels))\nplt.show()\n\nA batch from the DataLoader:\n\n\n\n\n\n\n\n\n\n\n\nModel must learn to focus on the central digit and ignore the extra context\n\nclasses = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n\nprint(\"Step 2: Model Definition\")\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(SimpleCNN, self).__init__()\n        # SVHN images are 3x32x32 (3 color channels)\n        self.features = nn.Sequential(\n            # Input: 3x32x32\n            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1), # Output: 16x32x32\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2), # Output: 16x16x16\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), # Output: 32x16x16\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2) # Output: 32x8x8\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(32 * 8 * 8, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\nmodel = SimpleCNN(num_classes=10)\nprint(\"Model architecture:\")\nprint(model)\nprint(\"\\n\")\n\nStep 2: Model Definition\nModel architecture:\nSimpleCNN(\n  (features): Sequential(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2048, out_features=128, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=128, out_features=10, bias=True)\n  )\n)\n\n\n\n\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nNUM_EPOCHS = 20\nfor epoch in range(NUM_EPOCHS):\n    model.train() \n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {running_loss/len(train_loader):.4f}\")\n\nprint(\"Training finished.\\n\")\n\nEpoch [1/20], Loss: 0.9173\nEpoch [2/20], Loss: 0.4778\nEpoch [3/20], Loss: 0.4011\nEpoch [4/20], Loss: 0.3499\nEpoch [5/20], Loss: 0.3119\nEpoch [6/20], Loss: 0.2827\nEpoch [7/20], Loss: 0.2563\nEpoch [8/20], Loss: 0.2327\nEpoch [9/20], Loss: 0.2116\nEpoch [10/20], Loss: 0.1927\nEpoch [11/20], Loss: 0.1746\nEpoch [12/20], Loss: 0.1595\nEpoch [13/20], Loss: 0.1442\nEpoch [14/20], Loss: 0.1314\nEpoch [15/20], Loss: 0.1208\nEpoch [16/20], Loss: 0.1081\nEpoch [17/20], Loss: 0.1003\nEpoch [18/20], Loss: 0.0915\nEpoch [19/20], Loss: 0.0826\nEpoch [20/20], Loss: 0.0747\nTraining finished.\n\n\n\n\nmodel.eval() \nall_preds = []\nall_labels = []\n\n# `torch.no_grad()` disables gradient calculation for efficiency\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        all_preds.extend(predicted.numpy())\n        all_labels.extend(labels.numpy())\n\naccuracy = accuracy_score(all_labels, all_preds)\nprecision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\nprint(f\"Validation Precision: {precision:.4f}\")\nprint(f\"Validation Recall: {recall:.4f}\")\nprint(f\"Validation F1 Score: {f1:.4f}\\n\")\n\nValidation Accuracy: 0.8778\nValidation Precision: 0.8787\nValidation Recall: 0.8778\nValidation F1 Score: 0.8778\n\n\n\n\n# model.state_dict()\n\n\nprint(\"Step 5: Saving and Loading the Model\")\n\nMODEL_PATH = \"svhn_cnn_weights.pth\"\ntorch.save(model.state_dict(), MODEL_PATH)\nprint(f\"Model saved to {MODEL_PATH}\")\n\nStep 5: Saving and Loading the Model\nModel saved to svhn_cnn_weights.pth\n\n\n\nnew_model = SimpleCNN(num_classes=10)\nnew_model.load_state_dict(torch.load(MODEL_PATH))\nprint(\"Model weights loaded into a new instance.\\n\")\n\nModel weights loaded into a new instance.\n\n\n\n\nprint(\"Step 6: Inference on New Data\")\n\nnew_data_point, true_label_idx = val_dataset[0]\nnew_data_point = new_data_point.unsqueeze(0)\n\nnew_model.eval()\n\nwith torch.no_grad():\n    prediction_logits = new_model(new_data_point)\n    # Get the class with the highest score\n    predicted_class_idx = torch.argmax(prediction_logits, dim=1).item()\n\nprint(f\"True Label: {classes[true_label_idx]}\")\nprint(f\"Predicted Class: {classes[predicted_class_idx]}\")\n\nStep 6: Inference on New Data\nTrue Label: 5\nPredicted Class: 5"
  }
]