{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Understanding Gram-Schmidt\"\n",
    "description: \"The Math and Code Behind Orthonormalization\"\n",
    "date: 2024-07-15\n",
    "categories: [vectors, machine-learning, statistics, numpy]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Objective](#objective)\n",
    "2. [Introduction](#introduction)\n",
    "3. [Obtaining Orthogonal Vectors](#orthogonal)\n",
    "4. [Normalization](#normalization)\n",
    "5. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='objective'></a>\n",
    "### Objective\n",
    "\n",
    "- Start with 2 independent vectors a and b\n",
    "- find orthonormal vectors q1 and q2 that span the same plane\n",
    "- find orthogonal vectors first A, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "### Introduction\n",
    "In linear algebra and data science, having a set of orthonormal vectors is incredibly useful. They form a basis that simplifies calculations and is the backbone of powerful techniques like Principal Component Analysis (PCA) and QR decomposition. But what if the starting vectors aren't orthonormal? That's where the Gram-Schmidt algorithm comes in. \n",
    "\n",
    "**Orthogonal Vectors**: if their dot product is 0 \\\n",
    "**Orthonormal Vectors**: if their dot product is 0 and their lengths are 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with two vectors, `a` and `b`. Our goal is to find two new vectors, `q1` and `q2`, that are orthonormal but span the exact same plane as `a` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) (3,)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,1,1])\n",
    "b = np.array([1,0,2])\n",
    "print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='orthogonal'></a>\n",
    "### Obtaining Orthogonal Vectors\n",
    "We'll pick our first vector, `a`, and call it `A`. This is our first basis vector. Now, for the second vector `b`, we need to make it orthogonal to `A`. We do this by calculating the 'shadow' that `b` casts on `A` (its projection) and subtracting it from `b`. What's left is a new vector, `B`, that is perfectly perpendicular (orthogonal) to `A`.\n",
    "\n",
    "Mathematically,\n",
    "$$\n",
    "A=a \\\\ \\\\\n",
    "B = b- \\frac{A^Tb}{A^TA}A\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the above equations in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getB(A, b):\n",
    "    \n",
    "    num = np.dot(A.T, b)\n",
    "    den = np.dot(A.T, A)\n",
    "    frac = num/den \n",
    "    B = b - np.dot(frac, A)\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[1 1 1],\n",
      "B:\n",
      "[ 0. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "A = a \n",
    "B = getB(A, b)\n",
    "print(f'A:\\n{A},\\nB:\\n{B}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product is 0.0!\n",
      "Length of A is 1.7320508075688772\n",
      "Length of B is 1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "dot_product = np.dot(A, B)\n",
    "print(f'Dot product is {dot_product}!')\n",
    "\n",
    "# length of vector\n",
    "len_A = np.linalg.norm(A, ord=2)\n",
    "len_B = np.linalg.norm(B, ord=2)\n",
    "print(f'Length of A is {len_A}\\nLength of B is {len_B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product is 0.0, which confirms `A` and `B` are **orthogonal**! However, we can see their lengths are not 1. This means they are an **orthogonal basis**, but not yet an **orthonormal one**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.57735027, 0.57735027, 0.57735027]), 1.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A/np.linalg.norm(A, ord=2), np.linalg.norm(A/np.linalg.norm(A, ord=2), ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='normalization'></a>\n",
    "### Normalization\n",
    "To make our vectors have a length of 1, we simply divide each vector by its own magnitude (its L2 norm). It is called normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q1 = \\frac{A}{||A||} \\\\\n",
    "Q2 = \\frac{B}{||B||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting orthonormal vectors \n",
    "def getQ(A, B):\n",
    "    return A/np.linalg.norm(A, ord=2), B/np.linalg.norm(B, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1:\n",
      "[0.57735027 0.57735027 0.57735027],\n",
      "Q2:\n",
      "[ 0.         -0.70710678  0.70710678]\n"
     ]
    }
   ],
   "source": [
    "Q1, Q2 = getQ(A, B)\n",
    "\n",
    "print(f'Q1:\\n{Q1},\\nQ2:\\n{Q2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product is 0.0!\n",
      "Length of Q1 is 1.0\n",
      "Length of Q2 is 1.0\n"
     ]
    }
   ],
   "source": [
    "dot_product = np.round(np.dot(Q1, Q2), decimals=3)\n",
    "print(f'Dot product is {dot_product}!')\n",
    "\n",
    "# length of vector\n",
    "len_Q1 = np.round(np.linalg.norm(Q1, ord=2), decimals=3)\n",
    "len_Q2 = np.round(np.linalg.norm(Q2, ord=2), decimals=3)\n",
    "print(f'Length of Q1 is {len_Q1}\\nLength of Q2 is {len_Q2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! The dot product of `Q1` and `Q2` is `0`, and their lengths are both `1`. We have successfully converted our initial vectors into an **orthonormal basis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "### References:\n",
    "\n",
    "- [Orthogonal Matrices: MIT Linear Algebra 2017 ](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/7ac32be444c25e48590f47573833ccc6_MIT18_06SCF11_Ses2.4sum.pdf)\n",
    "- [Orthogonal and Orthonormal Vectors: James McCaffrey Wordpress](https://jamesmccaffrey.wordpress.com/2019/12/14/the-difference-between-orthogonal-and-orthonormal-vectors/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
